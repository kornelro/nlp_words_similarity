{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('nlp_venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ab71aad47b39ca4a71e24b8ccab90c61f6d19b3eab8f6fc6fea9e795fef6b71a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import plwn\n",
    "import networkx as nx\n",
    "from networkx.algorithms.cycles import simple_cycles\n",
    "from networkx.algorithms.lowest_common_ancestors import lowest_common_ancestor\n",
    "from networkx.algorithms.minors import contracted_nodes\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "source": [
    "## Load wordnet to graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plwn.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = plwn.load('./data/default_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn.to_graphml(out_file='./data/graph_synset.xml', graph_type='synset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('./data/graph_synset.xml')"
   ]
  },
  {
   "source": [
    "## Describe graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes: 340647\n",
      "Edges: 1438540\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {len(G.nodes)}')\n",
    "print(f'Edges: {len(G.edges)}')"
   ]
  },
  {
   "source": [
    "## Load SimLex999 dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id         word1         word2  similarity  relatedness\n",
       "0      1         stary          nowy        0.43         7.29\n",
       "1      2        bystry  inteligentny        8.86         9.71\n",
       "2      3        ciężki        trudny        4.86         7.29\n",
       "3      4    szczęśliwy       radosny        8.14         8.86\n",
       "4      5         łatwy       męczący        0.43         6.43\n",
       "..   ...           ...           ...         ...          ...\n",
       "994  995      dołączyć        zdobyć        0.43         2.29\n",
       "995  996       wysyłać  uczestniczyć        0.00         0.86\n",
       "996  997       zbierać  uczestniczyć        0.00         0.71\n",
       "997  998     pochłonąć       wycofać        0.00         0.57\n",
       "998  999  uczestniczyć       przybyć        0.57         3.43\n",
       "\n",
       "[999 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>similarity</th>\n      <th>relatedness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>stary</td>\n      <td>nowy</td>\n      <td>0.43</td>\n      <td>7.29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>bystry</td>\n      <td>inteligentny</td>\n      <td>8.86</td>\n      <td>9.71</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>ciężki</td>\n      <td>trudny</td>\n      <td>4.86</td>\n      <td>7.29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>szczęśliwy</td>\n      <td>radosny</td>\n      <td>8.14</td>\n      <td>8.86</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>łatwy</td>\n      <td>męczący</td>\n      <td>0.43</td>\n      <td>6.43</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>995</td>\n      <td>dołączyć</td>\n      <td>zdobyć</td>\n      <td>0.43</td>\n      <td>2.29</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>996</td>\n      <td>wysyłać</td>\n      <td>uczestniczyć</td>\n      <td>0.00</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>997</td>\n      <td>zbierać</td>\n      <td>uczestniczyć</td>\n      <td>0.00</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>998</td>\n      <td>pochłonąć</td>\n      <td>wycofać</td>\n      <td>0.00</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>999</td>\n      <td>uczestniczyć</td>\n      <td>przybyć</td>\n      <td>0.57</td>\n      <td>3.43</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "simlex = pd.read_csv('./data/MSimLex999_Polish.txt', sep='\\t', header=None)\n",
    "simlex.columns = ['id', 'word1', 'word2', 'similarity', 'relatedness']\n",
    "simlex"
   ]
  },
  {
   "source": [
    "## Create word: id json"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "unique_words = set(list(simlex['word1'].unique()) + list(simlex['word2'].unique()))\n",
    "\n",
    "len(list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1139/1139 [09:01<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# ids_dict = {}\n",
    "# for word in tqdm(unique_words):\n",
    "#     synset_id = None\n",
    "#     try:\n",
    "#         synset_id = wn.synset(word, plwn.PoS.noun, 1).to_dict()['id']\n",
    "#     except plwn.exceptions.SynsetNotFound:\n",
    "#         pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.verb, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.adjective, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.adverb, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     \n",
    "#     if synset_id:\n",
    "#         ids_dict[word] = synset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/ids_dict.json', 'w', encoding='utf8') as f:\n",
    "#    json.dump(ids_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ids_dict.json', 'r', encoding='utf8') as f:\n",
    "    ids_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(ids_dict)"
   ]
  },
  {
   "source": [
    "## Create subgraph - graph filtering by 'hiponimia'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1438540/1438540 [00:00<00:00, 1720887.95it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_edges = [edge for edge in tqdm(list(G.edges)) if edge[2].endswith('hiperonimia')] # if (edge[2].endswith('hiperonimia')) or (edge[2].endswith('hiponimia'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 204667/204667 [00:00<00:00, 218170.04it/s]\n"
     ]
    }
   ],
   "source": [
    "subG = nx.DiGraph()\n",
    "for edge in tqdm(filtered_edges):\n",
    "    subG.add_edge(edge[0], edge[1], label=edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes: 173726 - part of G: 0.50999\n",
      "Edges: 204667 - part of G: 0.14227\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {len(subG.nodes)} - part of G: {round(len(subG.nodes)/len(G.nodes), 5)}')\n",
    "print(f'Edges: {len(subG.edges)} - part of G: {round(len(subG.edges)/len(G.edges), 5)}')"
   ]
  },
  {
   "source": [
    "## Removing cycles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nCYCLE\nID \t WORD \t\tIN_SIMLEX\n2373 \t podjąć  \tFalse\n2497 \t zająć się  \tFalse\n2355 \t zrobić  \tTrue\n44782 \t zacząć  \tFalse\n\nCYCLE\nID \t WORD \t\tIN_SIMLEX\n2496 \t zajmować się  \tFalse\n55305 \t robić  \tTrue\n2367 \t podejmować  \tFalse\n"
     ]
    }
   ],
   "source": [
    "cycles = list(simple_cycles(subG))\n",
    "\n",
    "for cycle in cycles:\n",
    "    print('\\nCYCLE')\n",
    "    print('ID \\t WORD \\t\\tIN_SIMLEX')\n",
    "    for node in cycle:\n",
    "        word = wn.synset_by_id(node).to_dict()['units'][0]['lemma']\n",
    "        in_simlex = int(node) in list(ids_dict.values())\n",
    "        print(f'{node} \\t {word}  \\t{in_simlex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_node = '2355'\n",
    "false_nodes = ['2373', '44782', '2497']\n",
    "for false_node in false_nodes:\n",
    "    subG = contracted_nodes(subG, true_node, false_node, self_loops=False)\n",
    "\n",
    "true_node = '55305'\n",
    "false_nodes = ['2496', '2367']\n",
    "for false_node in false_nodes:\n",
    "    subG = contracted_nodes(subG, true_node, false_node, self_loops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "len(list(simple_cycles(subG)))"
   ]
  },
  {
   "source": [
    "## Adding main root node"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4114\n1\n"
     ]
    }
   ],
   "source": [
    "root_nodes = [k for (k, v) in subG.in_degree() if v == 0]\n",
    "print(len(root_nodes))\n",
    "\n",
    "for node in root_nodes:\n",
    "    subG.add_edge('root', node)\n",
    "\n",
    "root_nodes = [k for (k, v) in subG.in_degree() if v == 0]\n",
    "print(len(root_nodes))"
   ]
  },
  {
   "source": [
    "## Wu and Palmer’s Conceptual Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(node_1, node_2):\n",
    "    lso = lowest_common_ancestor(subG, node_1, node_2)\n",
    "\n",
    "    a = 2 * len(nx.shortest_path(subG, 'root', lso))\n",
    "    b = len(nx.shortest_path(subG.to_undirected(), node_1, lso)) + len(nx.shortest_path(subG.to_undirected(), node_2, lso)) + a\n",
    "\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# TEST\n",
    "wu_palmer('262143', '1726')"
   ]
  },
  {
   "source": [
    "## Leacock and Chodorow’s Normalized Path Length"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 173722/173722 [03:22<00:00, 858.54it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes_depth = [(node, len(nx.shortest_path(subG, 'root', node))) for node in tqdm(subG.nodes())]\n",
    "nodes_depth.sort(key=lambda el: el[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "max_depth = nodes_depth[-1][1]\n",
    "\n",
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_normalized_path(node_1, node_2):\n",
    "    a = len(nx.shortest_path(subG.to_undirected(), node_1, node_2))\n",
    "    b = 2 * max_depth\n",
    "\n",
    "    return math.log(a/b) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.332204510175204"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "# TEST\n",
    "lc_normalized_path('262143', '1726')"
   ]
  },
  {
   "source": [
    "## Counting similarities and addig to dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_graph = set(ids_dict.keys())\n",
    "filtered_simlex = simlex[(simlex['word1'].isin(words_in_graph)) & (simlex['word2'].isin(words_in_graph))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wp(simlex_row):\n",
    "    node_1 = str(ids_dict[simlex_row['word1']])\n",
    "    node_2 = str(ids_dict[simlex_row['word2']])\n",
    "\n",
    "    try:\n",
    "        simlex_row['wu_palmer'] = wu_palmer(node_1, node_2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        simlex_row['wu_palmer'] = None\n",
    "\n",
    "    return simlex_row\n",
    "\n",
    "\n",
    "def add_lcn(simlex_row):\n",
    "    node_1 = str(ids_dict[simlex_row['word1']])\n",
    "    node_2 = str(ids_dict[simlex_row['word2']])\n",
    "\n",
    "    try:\n",
    "        simlex_row['leacon'] = lc_normalized_path(node_1, node_2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        simlex_row['leacon'] = None\n",
    "\n",
    "    return simlex_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex = filtered_simlex.progress_apply(add_wp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex = filtered_simlex.progress_apply(add_lcn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex = filtered_simlex.dropna().reset_index(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex.to_csv('out/wordnet_reslts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pies.1(21:zw)\n\n\nssak z rodziny psowatych.\n\n\n()\n\n\n{'id': 5563, 'lemma': 'pies', 'pos': 'noun', 'variant': 1, 'definition': 'ssak z rodziny psowatych.', 'sense_examples': ('Zmysł powonienia lisów jest dobry, lecz słabszy od węchu innych psów.',), 'sense_examples_sources': ('P',), 'external_links': (), 'usage_notes': ('specj.',), 'domain': 'zwierzęta', 'synset': 5194, 'verb_aspect': None, 'emotion_markedness': None, 'emotion_names': (), 'emotion_valuations': (), 'emotion_example': None, 'emotion_example_secondary': None, 'str': 'pies.1(21:zw)', 'related': {}}\n"
     ]
    }
   ],
   "source": [
    "lex = wn.lexical_unit('pies', plwn.PoS.noun_pl, 1)\n",
    "print(lex)\n",
    "print('\\n')\n",
    "print(lex.definition)\n",
    "print('\\n')\n",
    "print(lex.related())\n",
    "print('\\n')\n",
    "print(lex.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'id': 5194, 'definition': '', 'is_artificial': False, 'units': ({'id': 5563, 'lemma': 'pies', 'pos': 'noun', 'variant': 1, 'definition': 'ssak z rodziny psowatych.', 'sense_examples': ('Zmysł powonienia lisów jest dobry, lecz słabszy od węchu innych psów.',), 'sense_examples_sources': ('P',), 'external_links': (), 'usage_notes': ('specj.',), 'domain': 'zwierzęta', 'synset': 5194, 'verb_aspect': None, 'emotion_markedness': None, 'emotion_names': (), 'emotion_valuations': (), 'emotion_example': None, 'emotion_example_secondary': None, 'str': 'pies.1(21:zw)', 'related': {}},), 'str': '{pies.1(21:zw)}', 'related': {'egzemplarz': ((72555, '{Cerber.1(21:zw)}'),), 'fuzzynimia_synsetów': ((80494, '{dogoterapia.1(2:czy), [+ 1 unit(s)]}'), (7061418, '{kabanos.2(10:jedz), [+ 1 unit(s)]}')), 'hiperonimia': ((256921, '{wilk_indyjski.1(21:zw)}'), (394589, '{pies.4(21:zw)}'), (34120, '{likaon.1(21:zw), [+ 1 unit(s)]}'), (256922, '{wilk_rudy.1(21:zw), [+ 1 unit(s)]}'), (27423, '{kojot.1(21:zw), [+ 2 unit(s)]}'), (257017, '{wilk_falklandzki.1(21:zw)}'), (256920, '{wilk_himalajski.1(21:zw)}'), (256924, '{szakal.2(21:zw)}'), (256918, '{dingo.1(21:zw)}'), (34160, '{pies.2(21:zw), [+ 1 unit(s)]}'), (257012, '{wilczek_krótkouchy.1(21:zw)}'), (12886, '{lis.1(21:zw)}'), (34510, '{suka.2(21:zw)}'), (3801, '{wilk.1(21:zw), [+ 1 unit(s)]}'), (257015, '{wilk_andyjski.1(21:zw), [+ 1 unit(s)]}'), (256927, '{cyjon.1(21:zw)}'), (257030, '{pies_leśny.1(21:zw)}'), (257029, '{wilk_grzywiasty.1(21:zw)}'), (257031, '{otocjon.1(21:zw)}'), (34108, '{jenot.1(21:zw), [+ 5 unit(s)]}')), 'hiponimia': ((254595, '{ssak_drapieżny.1(21:zw)}'),), 'meronimia/element taksonomiczny': ((73998, '{psowate.1(46:sys)}'),), 'synonimia_międzyjęzykowa/Syn_plWN-PWN': ((295618, '{canine.2(21:zw), [+ 1 unit(s)]}'),)}}\n\n\n<bound method Synset.related_pairs of <Synset id=5194 lemma='pies' pos=<PoS.noun: 'noun'> variant=1>>\n"
     ]
    }
   ],
   "source": [
    "synset = wn.synset('pies', plwn.PoS.noun_pl, 1)\n",
    "print(synset.to_dict())\n",
    "print('\\n')\n",
    "print(synset.related_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}