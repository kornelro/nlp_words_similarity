{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ab71aad47b39ca4a71e24b8ccab90c61f6d19b3eab8f6fc6fea9e795fef6b71a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import plwn\n",
    "import networkx as nx\n",
    "from networkx.algorithms.cycles import simple_cycles\n",
    "from networkx.algorithms.lowest_common_ancestors import lowest_common_ancestor\n",
    "from networkx.algorithms.minors import contracted_nodes\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "source": [
    "## Load wordnet to graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plwn.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = plwn.load('./data/default_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn.to_graphml(out_file='./data/graph_synset.xml', graph_type='synset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('./data/graph_synset.xml')"
   ]
  },
  {
   "source": [
    "## Describe graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes: 340647\n",
      "Edges: 1438540\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {len(G.nodes)}')\n",
    "print(f'Edges: {len(G.edges)}')"
   ]
  },
  {
   "source": [
    "## Load SimLex999 dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id         word1         word2  similarity  relatedness\n",
       "0      1         stary          nowy        0.43         7.29\n",
       "1      2        bystry  inteligentny        8.86         9.71\n",
       "2      3        ciężki        trudny        4.86         7.29\n",
       "3      4    szczęśliwy       radosny        8.14         8.86\n",
       "4      5         łatwy       męczący        0.43         6.43\n",
       "..   ...           ...           ...         ...          ...\n",
       "994  995      dołączyć        zdobyć        0.43         2.29\n",
       "995  996       wysyłać  uczestniczyć        0.00         0.86\n",
       "996  997       zbierać  uczestniczyć        0.00         0.71\n",
       "997  998     pochłonąć       wycofać        0.00         0.57\n",
       "998  999  uczestniczyć       przybyć        0.57         3.43\n",
       "\n",
       "[999 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>similarity</th>\n      <th>relatedness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>stary</td>\n      <td>nowy</td>\n      <td>0.43</td>\n      <td>7.29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>bystry</td>\n      <td>inteligentny</td>\n      <td>8.86</td>\n      <td>9.71</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>ciężki</td>\n      <td>trudny</td>\n      <td>4.86</td>\n      <td>7.29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>szczęśliwy</td>\n      <td>radosny</td>\n      <td>8.14</td>\n      <td>8.86</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>łatwy</td>\n      <td>męczący</td>\n      <td>0.43</td>\n      <td>6.43</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>995</td>\n      <td>dołączyć</td>\n      <td>zdobyć</td>\n      <td>0.43</td>\n      <td>2.29</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>996</td>\n      <td>wysyłać</td>\n      <td>uczestniczyć</td>\n      <td>0.00</td>\n      <td>0.86</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>997</td>\n      <td>zbierać</td>\n      <td>uczestniczyć</td>\n      <td>0.00</td>\n      <td>0.71</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>998</td>\n      <td>pochłonąć</td>\n      <td>wycofać</td>\n      <td>0.00</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>999</td>\n      <td>uczestniczyć</td>\n      <td>przybyć</td>\n      <td>0.57</td>\n      <td>3.43</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "simlex = pd.read_csv('./data/MSimLex999_Polish.txt', sep='\\t', header=None)\n",
    "simlex.columns = ['id', 'word1', 'word2', 'similarity', 'relatedness']\n",
    "simlex"
   ]
  },
  {
   "source": [
    "## Create word: id json"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "unique_words = set(list(simlex['word1'].unique()) + list(simlex['word2'].unique()))\n",
    "\n",
    "len(list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_dict = {}\n",
    "# for word in tqdm(unique_words):\n",
    "#     synset_id = None\n",
    "#     try:\n",
    "#         synset_id = wn.synset(word, plwn.PoS.noun, 1).to_dict()['id']\n",
    "#     except plwn.exceptions.SynsetNotFound:\n",
    "#         pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.verb, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.adjective, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     if not synset_id:\n",
    "#         try:\n",
    "#             synset_id = wn.synset(word, plwn.PoS.adverb, 1).to_dict()['id']\n",
    "#         except plwn.exceptions.SynsetNotFound:\n",
    "#             pass\n",
    "#     \n",
    "#     if synset_id:\n",
    "#         ids_dict[word] = synset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/ids_dict.json', 'w', encoding='utf8') as f:\n",
    "#    json.dump(ids_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ids_dict.json', 'r', encoding='utf8') as f:\n",
    "    ids_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(ids_dict)"
   ]
  },
  {
   "source": [
    "## Create subgraph - graph filtering by 'hiponimia'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1438540/1438540 [00:01<00:00, 1386817.19it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_edges = [edge for edge in tqdm(list(G.edges)) if edge[2].endswith('hiperonimia')] # if (edge[2].endswith('hiperonimia')) or (edge[2].endswith('hiponimia'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 204667/204667 [00:01<00:00, 158183.82it/s]\n"
     ]
    }
   ],
   "source": [
    "subG = nx.DiGraph()\n",
    "for edge in tqdm(filtered_edges):\n",
    "    subG.add_edge(edge[0], edge[1], label=edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nodes: 173726 - part of G: 0.50999\n",
      "Edges: 204667 - part of G: 0.14227\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {len(subG.nodes)} - part of G: {round(len(subG.nodes)/len(G.nodes), 5)}')\n",
    "print(f'Edges: {len(subG.edges)} - part of G: {round(len(subG.edges)/len(G.edges), 5)}')"
   ]
  },
  {
   "source": [
    "## Removing cycles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "CYCLE\n",
      "ID \t WORD \t\tIN_SIMLEX\n",
      "2373 \t podjąć  \tFalse\n",
      "2497 \t zająć się  \tFalse\n",
      "2355 \t zrobić  \tTrue\n",
      "44782 \t zacząć  \tFalse\n",
      "\n",
      "CYCLE\n",
      "ID \t WORD \t\tIN_SIMLEX\n",
      "2496 \t zajmować się  \tFalse\n",
      "55305 \t robić  \tTrue\n",
      "2367 \t podejmować  \tFalse\n"
     ]
    }
   ],
   "source": [
    "cycles = list(simple_cycles(subG))\n",
    "\n",
    "for cycle in cycles:\n",
    "    print('\\nCYCLE')\n",
    "    print('ID \\t WORD \\t\\tIN_SIMLEX')\n",
    "    for node in cycle:\n",
    "        word = wn.synset_by_id(node).to_dict()['units'][0]['lemma']\n",
    "        in_simlex = int(node) in list(ids_dict.values())\n",
    "        print(f'{node} \\t {word}  \\t{in_simlex}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_node = '2355'\n",
    "false_nodes = ['2373', '44782', '2497']\n",
    "for false_node in false_nodes:\n",
    "    subG = contracted_nodes(subG, true_node, false_node, self_loops=False)\n",
    "\n",
    "true_node = '55305'\n",
    "false_nodes = ['2496', '2367']\n",
    "for false_node in false_nodes:\n",
    "    subG = contracted_nodes(subG, true_node, false_node, self_loops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(list(simple_cycles(subG)))"
   ]
  },
  {
   "source": [
    "## Adding main root node"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4114\n1\n"
     ]
    }
   ],
   "source": [
    "root_nodes = [k for (k, v) in subG.in_degree() if v == 0]\n",
    "print(len(root_nodes))\n",
    "\n",
    "for node in root_nodes:\n",
    "    subG.add_edge('root', node)\n",
    "\n",
    "root_nodes = [k for (k, v) in subG.in_degree() if v == 0]\n",
    "print(len(root_nodes))"
   ]
  },
  {
   "source": [
    "## Wu and Palmer’s Conceptual Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(node_1, node_2):\n",
    "    lso = lowest_common_ancestor(subG, node_1, node_2)\n",
    "\n",
    "    a = 2 * len(nx.shortest_path(subG, 'root', lso))\n",
    "    b = len(nx.shortest_path(subG.to_undirected(), node_1, lso)) + len(nx.shortest_path(subG.to_undirected(), node_2, lso)) + a\n",
    "\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13.396488904953003"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "# TEST\n",
    "start_tme = time.time()\n",
    "print(wu_palmer('262143', '1726'))\n",
    "exec_time = time.time() - start_tme\n",
    "\n",
    "exec_time"
   ]
  },
  {
   "source": [
    "## Leacock and Chodorow’s Normalized Path Length"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_root = nx.shortest_path_length(subG, 'root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "max_depth = max(sp_root.values()) + 1\n",
    "\n",
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_normalized_path(node_1, node_2):\n",
    "    a = len(nx.shortest_path(subG.to_undirected(), node_1, node_2))\n",
    "    b = 2 * max_depth\n",
    "\n",
    "    return math.log(a/b) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.332204510175204\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.09364914894104"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "# TEST\n",
    "start_tme = time.time()\n",
    "print(lc_normalized_path('262143', '1726'))\n",
    "exec_time = time.time() - start_tme\n",
    "\n",
    "exec_time"
   ]
  },
  {
   "source": [
    "## Counting similarities and addig to dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_graph = set(ids_dict.keys())\n",
    "filtered_simlex = simlex[(simlex['word1'].isin(words_in_graph)) & (simlex['word2'].isin(words_in_graph))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wp(simlex_row):\n",
    "    node_1 = str(ids_dict[simlex_row['word1']])\n",
    "    node_2 = str(ids_dict[simlex_row['word2']])\n",
    "\n",
    "    try:\n",
    "        simlex_row['wu_palmer'] = wu_palmer(node_1, node_2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        simlex_row['wu_palmer'] = None\n",
    "\n",
    "    return simlex_row\n",
    "\n",
    "\n",
    "def add_lcn(simlex_row):\n",
    "    node_1 = str(ids_dict[simlex_row['word1']])\n",
    "    node_2 = str(ids_dict[simlex_row['word2']])\n",
    "\n",
    "    try:\n",
    "        simlex_row['leacon'] = lc_normalized_path(node_1, node_2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        simlex_row['leacon'] = None\n",
    "\n",
    "    return simlex_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/941 [00:06<47:39,  3.05s/it]The node 49781 is not in the digraph.\n",
      "  3%|▎         | 27/941 [05:03<2:35:15, 10.19s/it]The node 49781 is not in the digraph.\n",
      " 16%|█▌        | 148/941 [30:56<2:16:48, 10.35s/it]The node 47608 is not in the digraph.\n",
      " 18%|█▊        | 165/941 [34:14<2:09:48, 10.04s/it]The node 6007 is not in the digraph.\n",
      " 19%|█▉        | 182/941 [37:42<2:18:36, 10.96s/it]The node 6007 is not in the digraph.\n",
      " 20%|█▉        | 184/941 [38:02<2:06:21, 10.02s/it]The node 245559 is not in the digraph.\n",
      " 20%|██        | 189/941 [39:03<2:14:22, 10.72s/it]The node 7074317 is not in the digraph.\n",
      " 28%|██▊       | 263/941 [54:37<1:54:28, 10.13s/it]The node 15636 is not in the digraph.\n",
      " 29%|██▉       | 275/941 [56:57<1:53:06, 10.19s/it]The node 11412 is not in the digraph.\n",
      " 36%|███▌      | 335/941 [1:08:43<1:40:13,  9.92s/it]The node 1418 is not in the digraph.\n",
      " 40%|████      | 379/941 [1:17:13<1:31:38,  9.78s/it]The node 417999 is not in the digraph.\n",
      " 43%|████▎     | 408/941 [1:22:46<1:26:08,  9.70s/it]The node 6105 is not in the digraph.\n",
      " 44%|████▍     | 417/941 [1:24:25<1:23:54,  9.61s/it]The node 13868 is not in the digraph.\n",
      " 49%|████▉     | 463/941 [1:33:18<1:17:05,  9.68s/it]The node 14041 is not in the digraph.\n",
      " 50%|████▉     | 470/941 [1:34:35<1:16:26,  9.74s/it]The node 422938 is not in the digraph.\n",
      " 53%|█████▎    | 496/941 [1:39:33<1:13:50,  9.96s/it]The node 6270 is not in the digraph.\n",
      " 54%|█████▎    | 505/941 [1:41:11<1:09:21,  9.55s/it]The node 15595 is not in the digraph.\n",
      " 57%|█████▋    | 538/941 [1:47:32<1:04:52,  9.66s/it]The node 8081 is not in the digraph.\n",
      " 61%|██████    | 576/941 [1:54:51<59:29,  9.78s/it]  The node 15593 is not in the digraph.\n",
      " 62%|██████▏   | 582/941 [1:55:56<58:15,  9.74s/it]  The node 12754 is not in the digraph.\n",
      " 70%|███████   | 660/941 [2:11:02<46:09,  9.85s/it]The node 36386 is not in the digraph.\n",
      " 75%|███████▍  | 705/941 [2:19:43<37:46,  9.60s/it]The node 50945 is not in the digraph.\n",
      " 76%|███████▌  | 712/941 [2:20:59<37:27,  9.82s/it]The node 422938 is not in the digraph.\n",
      " 76%|███████▋  | 718/941 [2:22:02<35:03,  9.43s/it]The node 422938 is not in the digraph.\n",
      " 76%|███████▋  | 719/941 [2:22:08<30:46,  8.32s/it]The node 422938 is not in the digraph.\n",
      " 90%|█████████ | 847/941 [2:49:00<17:02, 10.88s/it]The node 60377 is not in the digraph.\n",
      " 93%|█████████▎| 873/941 [2:54:03<11:01,  9.72s/it]The node 5917 is not in the digraph.\n",
      " 99%|█████████▉| 934/941 [3:06:15<01:11, 10.21s/it]The node 5917 is not in the digraph.\n",
      "100%|██████████| 941/941 [3:07:54<00:00, 11.98s/it]\n"
     ]
    }
   ],
   "source": [
    "filtered_simlex = filtered_simlex.progress_apply(add_wp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 2/941 [00:02<16:17,  1.04s/it]Either source 49781 or target 235228 is not in G\n",
      "  3%|▎         | 27/941 [01:04<37:21,  2.45s/it]Either source 49781 or target 9853 is not in G\n",
      " 16%|█▌        | 148/941 [05:38<28:54,  2.19s/it]Either source 47608 or target 11641 is not in G\n",
      " 18%|█▊        | 165/941 [06:16<28:19,  2.19s/it]Either source 65383 or target 6007 is not in G\n",
      " 19%|█▉        | 182/941 [06:54<28:24,  2.25s/it]Either source 2854 or target 6007 is not in G\n",
      " 20%|█▉        | 184/941 [06:59<29:36,  2.35s/it]Either source 245559 or target 5544 is not in G\n",
      " 20%|██        | 189/941 [07:10<26:44,  2.13s/it]Either source 7074317 or target 17524 is not in G\n",
      " 28%|██▊       | 263/941 [09:56<26:07,  2.31s/it]Either source 351 or target 15636 is not in G\n",
      " 29%|██▉       | 275/941 [10:23<24:17,  2.19s/it]Either source 11412 or target 43485 is not in G\n",
      " 36%|███▌      | 335/941 [12:29<20:28,  2.03s/it]Either source 12911 or target 1418 is not in G\n",
      " 40%|████      | 379/941 [14:02<19:08,  2.04s/it]Either source 1202 or target 417999 is not in G\n",
      " 43%|████▎     | 408/941 [15:05<18:32,  2.09s/it]Either source 6098 or target 6105 is not in G\n",
      " 44%|████▍     | 417/941 [15:29<21:37,  2.48s/it]Either source 4620 or target 13868 is not in G\n",
      " 49%|████▉     | 463/941 [17:09<16:18,  2.05s/it]Either source 14041 or target 8050 is not in G\n",
      " 50%|████▉     | 470/941 [17:24<16:05,  2.05s/it]Either source 7742 or target 422938 is not in G\n",
      " 53%|█████▎    | 496/941 [18:19<15:06,  2.04s/it]Either source 6476 or target 6270 is not in G\n",
      " 54%|█████▎    | 505/941 [18:38<16:36,  2.29s/it]Either source 15595 or target 6730 is not in G\n",
      " 57%|█████▋    | 538/941 [19:48<15:18,  2.28s/it]Either source 32065 or target 8081 is not in G\n",
      " 61%|██████    | 576/941 [21:07<12:47,  2.10s/it]Either source 15593 or target 6699 is not in G\n",
      " 62%|██████▏   | 582/941 [21:20<13:34,  2.27s/it]Either source 6959 or target 12754 is not in G\n",
      " 70%|███████   | 660/941 [24:05<09:54,  2.12s/it]Either source 36386 or target 6298 is not in G\n",
      " 75%|███████▍  | 705/941 [25:46<08:41,  2.21s/it]Either source 7634 or target 50945 is not in G\n",
      " 76%|███████▌  | 712/941 [26:01<08:27,  2.22s/it]Either source 422938 or target 2328 is not in G\n",
      " 76%|███████▋  | 718/941 [26:15<08:24,  2.26s/it]Either source 422938 or target 15590 is not in G\n",
      " 76%|███████▋  | 719/941 [26:16<07:52,  2.13s/it]Either source 602 or target 422938 is not in G\n",
      " 90%|█████████ | 847/941 [30:47<03:21,  2.14s/it]Either source 3600 or target 60377 is not in G\n",
      " 93%|█████████▎| 873/941 [31:42<02:24,  2.12s/it]Either source 5917 or target 52693 is not in G\n",
      " 99%|█████████▉| 934/941 [33:52<00:15,  2.28s/it]Either source 59986 or target 5917 is not in G\n",
      "100%|██████████| 941/941 [34:08<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "filtered_simlex = filtered_simlex.progress_apply(add_lcn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex = filtered_simlex.dropna().reset_index(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_simlex.to_csv('out/wordnet_results.csv')"
   ]
  },
  {
   "source": [
    "## K top similar words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset_id(word):\n",
    "    return str(wn.synsets(word)[0].to_dict()['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_top(k, words, metric_fun, max=True):\n",
    "    \n",
    "    combs = list(itertools.combinations(words, 2))\n",
    "    metrics = {}\n",
    "    for comb in tqdm(combs):\n",
    "        id_1 = get_synset_id(comb[0])\n",
    "        id_2 = get_synset_id(comb[1])\n",
    "        metric = metric_fun(id_1, id_2)\n",
    "        metrics[str(set(comb))] = metric\n",
    "\n",
    "    results = {} # comb: metric\n",
    "\n",
    "    combs = list(itertools.combinations(words, k))\n",
    "    for comb in combs:\n",
    "        sub_combs = list(itertools.combinations(comb, 2))\n",
    "        metrics_to_mean = []\n",
    "        for sub_comb in sub_combs:\n",
    "            metrics_to_mean.append(\n",
    "                metrics[str(set(sub_comb))]\n",
    "            )\n",
    "        results[str(comb)] = np.mean(np.array(metrics_to_mean))\n",
    "\n",
    "    results = {k: v for k, v in sorted(results.items(), key=lambda item: item[1], reverse=max)}\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('TOP 1')\n",
    "    print('Słowa: ', list(results.items())[0][0])\n",
    "    print('Wartość metryki: ', round(list(results.items())[0][1], 4))\n",
    "    print()\n",
    "    print('TOP 2')\n",
    "    print('Słowa: ', list(results.items())[1][0])\n",
    "    print('Wartość metryki: ', round(list(results.items())[1][1], 4))\n",
    "    print()\n",
    "    print('TOP 3')\n",
    "    print('Słowa: ', list(results.items())[2][0])\n",
    "    print('Wartość metryki: ', round(list(results.items())[2][1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    'sufit',\n",
    "    'pochłonąć',\n",
    "    'okoliczność',\n",
    "    'rubież',\n",
    "    'upraszać',\n",
    "    'rytm',\n",
    "    'nowoczesny',\n",
    "    'pojemnik',\n",
    "    'gwałtowny',\n",
    "    'pudełko'\n",
    "]"
   ]
  },
  {
   "source": [
    "### WuPalmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 45/45 [10:03<00:00, 13.41s/it]\n",
      "\n",
      "TOP 1\n",
      "Słowa:  ('sufit', 'rubież')\n",
      "Wartość metryki:  0.4615\n",
      "\n",
      "TOP 2\n",
      "Słowa:  ('rytm', 'pojemnik')\n",
      "Wartość metryki:  0.2857\n",
      "\n",
      "TOP 3\n",
      "Słowa:  ('okoliczność', 'nowoczesny')\n",
      "Wartość metryki:  0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_top(2, words, wu_palmer, max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 45/45 [09:31<00:00, 12.71s/it]\n",
      "\n",
      "TOP 1\n",
      "Słowa:  ('sufit', 'okoliczność', 'rubież')\n",
      "Wartość metryki:  0.2751\n",
      "\n",
      "TOP 2\n",
      "Słowa:  ('sufit', 'rubież', 'nowoczesny')\n",
      "Wartość metryki:  0.2751\n",
      "\n",
      "TOP 3\n",
      "Słowa:  ('sufit', 'rubież', 'pojemnik')\n",
      "Wartość metryki:  0.2751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_top(3, words, wu_palmer, max=True)"
   ]
  },
  {
   "source": [
    "### LeacockChodorow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 45/45 [02:10<00:00,  2.90s/it]\n",
      "\n",
      "TOP 1\n",
      "Słowa:  ('sufit', 'rubież')\n",
      "Wartość metryki:  2.4159\n",
      "\n",
      "TOP 2\n",
      "Słowa:  ('sufit', 'okoliczność')\n",
      "Wartość metryki:  2.2336\n",
      "\n",
      "TOP 3\n",
      "Słowa:  ('sufit', 'pudełko')\n",
      "Wartość metryki:  2.2336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_top(2, words, lc_normalized_path, max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 45/45 [02:07<00:00,  2.84s/it]\n",
      "\n",
      "TOP 1\n",
      "Słowa:  ('sufit', 'rubież', 'pudełko')\n",
      "Wartość metryki:  2.2944\n",
      "\n",
      "TOP 2\n",
      "Słowa:  ('sufit', 'okoliczność', 'rubież')\n",
      "Wartość metryki:  2.243\n",
      "\n",
      "TOP 3\n",
      "Słowa:  ('sufit', 'okoliczność', 'pudełko')\n",
      "Wartość metryki:  2.1822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_top(3, words, lc_normalized_path, max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}